=== Chapter Skeletons

(The remainder of the intro section is a planning document, please skip)

==== Introductions

**Intros**:

Introduce the chapter to the reader

* take the strands from the last chapter, and show them braided together
* in this chapter, you'll learn .... OR ok we're done looking at that, now let's xxx
* weave in the locality question
* Tie each chapter to the goals of the book

* perspective, philosophy, what we'll be working, a bit of repositioning, a bit opinionated, a bit personal.

**Preface**:

* like a "map" of the book
* "First part is about blah, next is about blah, ..."

==== Skeleton: Storm+Trident Internals

What should you take away from this chapter:

You should:

* Understand the lifecycle of a Storm tuple, including spout, tupletree and acking.
* (Optional but not essential) Understand the details of its reliability mechanism and how tuples are acked.
* Understand the lifecycle of partitions within a Trident batch and thus, the context behind partition operations such as Apply or PartitionPersist.
* Understand Trident’s transactional mechanism, in the case of a PartitionPersist.
* Understand how Aggregators, Statemap and the Persistence methods combine to give you _exactly once_  processing with transactional guarantees.  Specifically, what an OpaqueValue record will look like in the database and why.
* Understand how the master batch coordinator and spout coordinator for the Kafka spout in particular work together to uniquely and efficiently process all records in a Kafka topic.
* One specific:  how Kafka partitions relate to Trident partitions.

==== Skeleton: Hadoop Internals

=====  HDFS

Lifecycle of a File:

* What happens as the Namenode and Datanode collaborate to create a new file.
* How that file is replicated to acknowledged by other Datanodes.
* What happens when a Datanode goes down or the cluster is rebalanced.
* Briefly, the S3 DFS facade // (TODO: check if HFS?).

===== Hadoop Job Execution

* Lifecycle of a job at the client level including figuring out where all the source data is; figuring out how to split it; sending the code to the JobTracker, then tracking it to completion.
* How the JobTracker and TaskTracker cooperate to run your job, including:  The distinction between Job, Task and Attempt., how each TaskTracker obtains its Attempts, and dispatches progress and metrics back to the JobTracker, how Attempts are scheduled, including what happens when an Attempt fails and speculative execution, ________, Split.
* How TaskTracker child and Datanode cooperate to execute an Attempt, including; what a child process is, making clear the distinction between TaskTracker and child process.
* Briefly, how the Hadoop Streaming child process works.

==== Skeleton: Map-Reduce Internals

* How the mapper and Datanode handle record splitting and how and when the partial records are dispatched.
* The mapper sort buffer and spilling to disk (maybe here or maybe later, the I/O.record.percent).
* Briefly note that data is not sent from mapper-to-reducer using HDFS and so you should pay attention to where you put the Map-Reduce scratch space and how stupid it is about handling an overflow volume.
* Briefly that combiners are a thing.
* Briefly how records are partitioned to reducers and that custom partitioners are a thing.
* How the Reducer accepts and tracks its mapper outputs.
* Details of the merge/sort (shuffle and sort), including the relevant buffers and flush policies and why it can skip the last merge phase.
* (NOTE:  Secondary sort and so forth will have been described earlier.)
* Delivery of output data to the HDFS and commit whether from mapper or reducer.
* Highlight the fragmentation problem with map-only jobs.
* Where memory is used, in particular, mapper-sort buffers, both kinds of reducer-merge buffers, application internal buffers.

==== Skeleton: Conceptual Model for Analytics.

===== Domain Boundaries.

* An interesting opportunity happens when the sequence order of records corresponds to one of your horizon keys.
* Explain using the example of weblogs. highlighting strict order and partial order.
* In the frequent case, the sequence order only somewhat corresponds to one of the horizon keys.  There are several types of somewhat ordered streams:  block disorder, bounded band disorder, band disorder.  When those conditions hold, you can use windows to recover the power you have with ordered streams -- often, without having to order the stream.
* Unbounded band disorder only allows "convergent truth" aggregators.  If you have no idea when or whether that some additional record from a horizon group might show up, then you can’t treat your aggregation as anything but a best possible guess at the truth.
* However, what the limited disorder does get you, is the ability to efficiently cache aggregations from  a practically infinite backing data store.
* With bounded band or block disorder, you can perform accumulator-style aggregations.
* How to, with the abstraction of an infinite sorting buffer or an infinite binning buffer, efficiently re-present the stream as one where sequence order and horizon key directly correspond.
* Re-explain the Hadoop Map-Reduce algorithm  in this window+horizon model.
* How windows and panes correspond to horizon groups, subgroups and the secondary sort; in particular, explain the CUBE and ROLLUP operations in Pig.
* (somewhere:  Describe how to use Trident batches as they currently stand to fake out windows.)

===== Fundamental Boundaries

* Understand why conceptual model is useful; in particular, it illuminates the core similarity between batch and stream analytics and also, to help you reason about the architecture of your analysis.
* The basic model:  Organize context globally, compute locally.  DO MORE HERE.
* Horizon of computation, including what we mean by horizon key.  DO MORE HERE.
* Volume of justified belief.  DO MORE HERE.
* Note that the direct motivation for Big Data technology is to address the situation where the necessary volume for justified belief exceeds the practical horizon of computation.
* Volume of aggregation, including holistic and algebraic aggregates.  Describe briefly one or two algebraic aggregates and two holistic aggregates, including medium (or something) and  Unified-Profile assembly.
* Highlight that , in practice, we often and eagerly trade off truth and accuracy in favor of relevance, timeliness, cost and the other constraints we’ve described.  Give a few examples.
* Timescale of acceptable delay.  DO MORE HERE.
* Timescale of syndication.  DO MORE HERE.
* Horizon of computational risk.  DO MORE HERE.
* Horizon of external conversation.  DO MORE HERE.

* Understand relativity: horizons of belief, computation, delay, etc
* How guarantees of bounded disorder or delay, uniform sampling, etc let you trade off
* Aggregation types: holistic, algebraic, combinable; accumulate, accretion

==== Skeleton: Geographic data

Continuous horizon: getting 1-d locality

==== Skeleton: Statistics

* Holistic vs algebraic aggregations
* Underflow and the "Law of Huge Numbers"
* Approximate holistic aggs: Median vs remedian; percentile; count distinct (hyperloglog)
* Count-min sketch for most frequent elements
* Approx histogram
